{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lab 2 Instructions\n",
    "\n",
    "In this notebook, we will learn how to do classification of MNIST handwritten digits using TensorFlow 2. The notebook contains all the info that you need to understand the basic mechanism of the classification theory. \n",
    "\n",
    "Your are free to tweak the hyper-parameters (including number of hidden units, number of hidden layers, learning rate, num of iterations and so on) to improve the performance of the model. The final block of the code computes the prediction accuracy of the model on the testing set, please do not change this block.\n",
    "\n",
    "Make sure that your final submission is a notebook that can be run from beginning to end, and you should print out the accuracy at the end of the notebook (i.e. be sure to run the last block after training). It is in fact possible to achieve >98% accuracy on this dataset with a more complex neural network architecture and careful tuning of hyper-parameters. **Your grade will depend on the final prediction accuracy**. However, if you tweak the evaluation code to report false result, you will receive no credit for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using TensorFlow 2\n",
    "\n",
    "In the introduction notebook, we walked through how to solve a supervised learning regression problem (i.e. where the labels are continuous values) from scratch. Now, you will build a model that solves a classification problem (i.e. where the labels are discrete values). We will use the MNIST hand written digit dataset, a toy benchmark for image classification models. Let's first load the dataset via the TensorFlow 2 API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Here $X_{train},Y_{train}$ denote the training data and $X_{test},Y_{test}$ denote the testing data. We train the model on training set and evaluate its performance on testing set (to evaluate potential under-fitting or over-fitting). As can be seen below, $X_{train}$ contains $60000$ examples with $28 \\times 28 $ pixels. $Y_{train}$ contains $60000$ corresponding examples with $10$ classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
    "\n",
    "\n",
    "train_dataloader = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(64)\n",
    "test_dataloader = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n",
    "\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = x_train[0], y_train[0]\n",
    "print(\"Shape of x: \", x.shape)\n",
    "print(\"Label: \", y)\n",
    "\n",
    "plt.imshow(x.squeeze(), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a bunch of samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "num_of_images = 60\n",
    "for i in range(1, num_of_images + 1):\n",
    "    x, y = x_train[i], y_train[i]\n",
    "    plt.subplot(6, 10, i)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "You are responsible for defining the model f(x) = y, where x is an image and y is the digit found in the image. We provide one way to do this, but you will need to explore different model architectures and training strategies to get sufficient performance: \n",
    "\n",
    "The shape of any image x is 28x28. We can reshape the image so it is instead a vector of length 28*28 = 784. Then, we can use the methods described in the introduction notebook to map a 784-dimensional vector to a 10-dimensional vector of probabilities. The digit in the image will then correspond to the index of the highest probability entry in the predicted 10-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "          tf.keras.layers.Dense(10),\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike regression, where the predicted output y is a scalar in $\\mathbb{R}$, in classification the model outputs logits or scores, one for each class. In our problem, there are 10 classes so the output of f(x) is a 10-dimensional vector of scores. \n",
    "\n",
    "In order to convert these scores to probabilities, we can simply normalize. Typically, the softmax operation is used to compute probabilties from logits or scores. After computing the prediction y = f(x), which is a 10-dimensional, we can compute the probability vector p:\n",
    "\n",
    "$$p_i = \\frac{e^{y_i}}{\\sum_{j=1}^{10} e^{y_j}}, i=1,\\ldots, 10, \\text{ or } p=softmax(y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilties_from_scores(y):\n",
    "    f = tf.nn.softmax\n",
    "    p = f(y)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "For $K$ classes, our labels y in the dataset will take on an integer value between [0,K]. We must convert these values to one-hot representations. For example, when $K=3$, y = 1 as a one-hot vector would be $[1,0,0]$ to represent the first class. If the model predicts a probability $p\\in\\mathbb{R}^K$ for this training instance, the loss function is\n",
    "\n",
    "$$L = - \\sum_{i=1}^K y_i \\log p_i$$\n",
    "\n",
    "Therefore, in the example where y = 1, L = - probability of the first entry since the one-hot vector is [1, 0, 0]. The loss function motivates assigning high probability to true classes and low probability otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat, y):\n",
    "    # y_hat has shape (N, 10) for 10 classes, y has shape (N,) \n",
    "    f = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    return f(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "Now we perform stochastic gradient descent on the model. We can first try to evaluate the model's initial performance on the testing dataset as a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, log=False):\n",
    "    correct_count, total_count = 0, 0\n",
    "    for images_batch, labels_batch in dataloader:\n",
    "        images = tf.reshape(images_batch, (images_batch.shape[0],28*28))\n",
    "        scores = model(images)\n",
    "        probabilities = probabilties_from_scores(scores)\n",
    "        pred_label = tf.math.argmax(probabilities, axis=1)\n",
    "        correct_count += tf.reduce_sum(tf.cast(pred_label == tf.cast(labels_batch, tf.int64), tf.int64)).numpy()\n",
    "        total_count += labels_batch.shape[0]\n",
    "\n",
    "    if log:\n",
    "        print(\"Number Of Images Tested =\", total_count)\n",
    "        print(\"Model Accuracy =\", (correct_count/total_count))\n",
    "    \n",
    "    return (correct_count/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(test_dataloader, model, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "Optimize the model to improve the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images_batch, labels_batch in train_dataloader:\n",
    "        images_batch = tf.reshape(images_batch, (images_batch.shape[0], 28*28))   # Flatten MNIST images into a 784 long vector\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # forward pass\n",
    "            y_hat = model(images_batch)\n",
    "            y = labels_batch\n",
    "            L = loss(y_hat, y)\n",
    "            \n",
    "        # backward pass\n",
    "        gradients = tape.gradient(L, model.trainable_variables)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        running_loss += L.numpy()\n",
    "        \n",
    "    training_loss = running_loss/len(train_dataloader)\n",
    "    train_accuracy = evaluate(train_dataloader, model)\n",
    "    test_accuracy = evaluate(test_dataloader, model)                                  \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch {} - Training loss: {}  Train Accuracy: {}  Test Accuracy: {}\".format(epoch, training_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation \n",
    "Test the model's accuracy on the unseen test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(test_dataloader, model, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the model's performance improves from 10% accuracy to about 92%. For this simple task, getting an accuracy of 92% is not quite impressive. Try to tweak the parameters and neural network architecture to get better predictions! You should be able to get >= 98% test accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
